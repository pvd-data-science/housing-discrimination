{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "#### top-level question:\n",
    "Do we see evidence of discrimination against housing vouchers?\n",
    "\n",
    "In Texas, discrimination against vouchers is enshrined into law, so the question shifts from \"is there discrimination\" to \"how does discrimination against vouchers play out in practice, and what might the effects be on tenants?\". While attempting to investigate this question, I was immediately derailed by the large number of spammy apartment locator postings among the actual apartment listings. These postings often directly reference Section 8 vouchers, and could bias an analysis of vouchers, so it is necessary to identify such postings.\n",
    "\n",
    "### immediate question: \n",
    "can I identify apartment locator postings among the actual apartment listings?\n",
    "\n",
    "#### outline:\n",
    "\n",
    "1. obtain the data by scraping craigslist\n",
    "2. explore its structure\n",
    "3. convert into DataFrame for analysis\n",
    "4. featurize text data\n",
    "5. explore possible clusters of postings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### imports & helper fxns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from requests import get\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "import random\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# URL = \"https://providence.craigslist.org/\"\n",
    "URL = \"https://dallas.craigslist.org/\"\n",
    "QUERY = \"search/apa\"\n",
    "\n",
    "DTYPE= 'craigslist-v1'\n",
    "\n",
    "def query_url():\n",
    "    return URL + QUERY\n",
    "\n",
    "def filter_line(line):\n",
    "    if not line or line == 'QR Code Link to This Post':\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "def map_line(line):\n",
    "    return line.strip('\\n').strip(' ')\n",
    "\n",
    "def get_description(href):\n",
    "    time.sleep(random.uniform(.2, .8))\n",
    "    response_details = get(href)\n",
    "    detail_soup = BeautifulSoup(response_details.text, 'html.parser')\n",
    "    body = detail_soup.find(id=\"postingbody\")\n",
    "    \n",
    "    return list(filter(filter_line, map(map_line, body.find_all(text=True))))\n",
    "\n",
    "def extract_repost(duplicate_rows):\n",
    "    if duplicate_rows:\n",
    "        return [row.get('data-repost-of') for row in duplicate_rows.find_all(class_=\"result-row\")]\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "def listing(post):\n",
    "    title = post.find(class_ = \"result-title hdrlnk\")\n",
    "    print(\"Getting: {}\".format(title.text))\n",
    "    return {\n",
    "        'price': post.find(class_ = \"result-price\").text, \n",
    "        'title': title.text,\n",
    "        'href': title.get('href'),\n",
    "        'description': get_description(title.get('href')),\n",
    "        'duplicates': extract_repost(post.find(class_=\"duplicate-rows\")),\n",
    "    }\n",
    "\n",
    "def extract_listings(postal_code, area):\n",
    "    time.sleep(random.uniform(.3, .8))\n",
    "    response = get(\n",
    "        query_url(), \n",
    "        params={\n",
    "            'bundleDuplicates':1, \n",
    "            'availabilityMode':0, \n",
    "            'sale_date':'all dates', \n",
    "            'search_distance': 0, \n",
    "            'postal': postal_code\n",
    "        }\n",
    "    )\n",
    "    html_soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    posts = html_soup.find_all('li', class_= ['result-row', 'result-row duplicate-row'])\n",
    "    dup_posts = html_soup.find_all('li', class_= 'result-row duplicate-row')\n",
    "\n",
    "    print(\n",
    "        \"Got type: {}; listings: {}; dup listings: {} for {}\".format(\n",
    "            type(posts), len(posts), len(dup_posts), postal_code)\n",
    "    )\n",
    "\n",
    "    # Is there pagination? Seems to match UI well but not exactly?\n",
    "    extracted = [listing(post) for post in posts + dup_posts]\n",
    "       \n",
    "    raw_path = os.path.join(DTYPE, 'raw_html')\n",
    "    os.makedirs(raw_path, exist_ok=True)\n",
    "    with open(os.path.join(raw_path, '{}-{}.html'.format(postal_code, area)), 'w', encoding='utf-8') as f:\n",
    "        f.write(response.text)\n",
    "\n",
    "    extracted_path = os.path.join(DTYPE, 'extracted')\n",
    "    os.makedirs(extracted_path, exist_ok=True)\n",
    "    with open(os.path.join(extracted_path, '{}-{}.json'.format(postal_code, area)), 'w', encoding='utf-8') as f:\n",
    "        json.dump(extracted, f, indent=2)\n",
    "        \n",
    "# extract_listings('02860', 'foo_bar, baz')\n",
    "        \n",
    "# for postal_code, area in dallas_area_postal_codes.items():\n",
    "#      extract_listings(postal_code, area)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### scrape/load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got type: <class 'bs4.element.ResultSet'>; listings: 156; dup listings: 10 for 75001\n",
      "Getting: LOVE the Place you call Home at Bent Tree Trails\n",
      "Getting: DON'T Miss Out on a Great Place to Call HOME!!!\n",
      "Getting: Upscale Townhome in North Dallas/Addison**Attached Garage!**\n",
      "Getting: Dishwasher, Limited Access Gates, Clothes Care Center\n",
      "Getting: Home Sweet Home at Bent Tree Trails\n",
      "Getting: Clubhouse, Covered Parking, Ice Maker\n",
      "Getting: We are conveniently located with walking distance to retail&Restaurant\n",
      "Getting: HOME SWEET HOME...\n",
      "Getting: Addison is the spot\n",
      "Getting: Free Premium Cable TV\n",
      "Getting: HOME SWEET HOME...\n",
      "Getting: Move in today.\n",
      "Getting: ðŸ’•FREE APARTMENT LOCATOR SERVICE THAT HELPS WITH 2ND CHANCE LEASING ðŸ’•\n",
      "Getting: Get a roof over your head TODAY - Immediate availability\n",
      "Getting: WELCOME TO YOUR NEW HOME\n",
      "Getting: Manager's Special $150 deposit\n",
      "Getting: Come live a while\n",
      "Getting: Come live a while\n",
      "Getting: Walk-In Closet, Upgraded Appliances, Controlled Access\n",
      "Getting: Tenemos cuartos listo hoy! Hablamos Espanol\n",
      "Getting: Tenemos cuartos listo hoy! Hablamos Espanol\n",
      "Getting: Get a roof over your head TODAY - Immediate availability\n",
      "Getting: WELCOME TO YOUR NEW HOME\n",
      "Getting: WELCOME TO YOUR NEW HOME\n",
      "Getting: Manager's Special $150 deposit\n",
      "Getting: Come live a while\n",
      "Getting: Come live a while\n",
      "Getting: 1 recamara y un bano empezando a $249 por semana\n",
      "Getting: Get a roof over your head TODAY - Immediate availability\n",
      "Getting: Fireplaces, Dishwasher, Ice Maker\n",
      "Getting: Close to Shopping, Clubhouse, Pet Friendly\n",
      "Getting: Walk-In Closet, Close to Restaurants, Spacious Floorplans\n",
      "Getting: Ice Maker, Pet Friendly, Dishwasher\n",
      "Getting: Close to Shopping, Clothes Care Center, Clubhouse\n",
      "Getting: %%%Newly remodeled master bathroom for rent\n",
      "Getting: On-Site Maintenance, Fireplaces, Swimming Pool\n",
      "Getting: Close to Restaurants, Spacious Floorplans, Clothes Care Center\n",
      "Getting: Fireplace, Fireplaces, LVT Flooring\n",
      "Getting: Clothes Care Center, Close to Restaurants, Mirrored Wardrobe\n",
      "Getting: Swimming Pool, Clothes Care Center, Clubhouse\n",
      "Getting: Tiled Entries, Spacious Floorplans, Mirrored Closet Doors\n",
      "Getting: Ice Maker, Tiled Entries, Mirrored Wardrobe\n",
      "Getting: 2 Swimming Pools, Business Center, Fitness Center\n",
      "Getting: Playground, Washer and dryer connection, Central A/C and heat, Yard\n",
      "Getting: Close to Freeway, Upgraded Appliances, Package Receiving\n",
      "Getting: Addison Luxury 2nd Chance Leasing - Bad Credit / Misdemeanor / Felony\n",
      "Getting: Fireplace, 2 Swimming Pools, Swimming Pool\n",
      "Getting: Yoga studio, Grilling areas, Demonstration kitchen, Business Center\n",
      "Getting: Swimming Pool, LVT Flooring, Ice Maker\n",
      "Getting: Controlled Access, Large Closet, Walk-In Closet\n",
      "Getting: Unit features warm color tones! Tour Today\n",
      "Getting: LVT Flooring, Fireplaces, Fitness Center\n",
      "Getting: Close to Airport, Covered Parking, LVT Flooring\n",
      "Getting: Have a room for rent.. Need roommate!\n",
      "Getting: Fireplace, Fireplaces, Controlled Access\n",
      "Getting: Section 8 | Second Chance | Broken Lease\n",
      "Getting: Upgraded Appliances, Swimming Pool, 2 Swimming Pools\n",
      "Getting: 3 bedroom available immediately! $250 off first full month!\n",
      "Getting: Fireplaces, Pantry, Pet Friendly\n",
      "Getting: 1bd 1bath AVAILABLE TODAY!\n",
      "Getting: Package Receiving, Courtyard, Dog Park\n",
      "Getting: Admin Waived!\n",
      "Getting: We are PET FRIENDLY!\n",
      "Getting: Pet Friendly, On-Site Maintenance, Spacious Floorplans\n",
      "Getting: Upscale Amenites! Monthly Social Events, Dry Cleaning!\n",
      "Getting: No Lease or Credit Check Required\n",
      "Getting: Free Premium Cable TV\n",
      "Getting: Newly Renovated, Move-In Ready & Awesome Specials.  Call Today!!\n",
      "Getting: Apartamentos con todos los biles pagados!\n"
     ]
    }
   ],
   "source": [
    "# I hand-labeled this set of zip codes at one point\n",
    "dallas_area_postal_codes = {\n",
    "    \"75023\": \"Plano\",\n",
    "    \"75024\": \"Plano\",\n",
    "    \"75025\": \"Plano\",\n",
    "    \"75040\": \"Garland\",\n",
    "    \"75041\": \"Garland\",\n",
    "    \"75042\": \"Garland\",\n",
    "    \"75043\": \"Garland\",\n",
    "    \"75044\": \"Garland\",\n",
    "    \"75048\": \"Garland\",\n",
    "    \"75074\": \"Plano\",\n",
    "    \"75075\": \"Plano\",\n",
    "    \"75080\": \"Richardson\",\n",
    "    \"75081\": \"Richardson\",\n",
    "    \"75082\": \"Richardson\",\n",
    "    \"75093\": \"Plano\",\n",
    "    \"75094\": \"Plano\",\n",
    "    \"75240\": \"Far North Dallas\",\n",
    "    \"75248\": \"Far North Dallas\",\n",
    "    \"75252\": \"Far North Dallas\",\n",
    "    \"75254\": \"Far North Dallas\",\n",
    "    \"75287\": \"Far North Dallas\",\n",
    "}\n",
    "\n",
    "# there are many more postal codes in the city of Dallas, so \n",
    "# copy entire list of zip codes from google search for \"dallas zip codes\":\n",
    "dallas_zip_codes = [\n",
    "    \"75001\",\n",
    "    \"75019\",\n",
    "    \"75032\",\n",
    "    \"75039\",\n",
    "    \"75043\",\n",
    "    \"75051\",\n",
    "    \"75052\",\n",
    "    \"75061\",\n",
    "    \"75063\",\n",
    "    \"75080\",\n",
    "    \"75087\",\n",
    "    \"75088\",\n",
    "    \"75089\",\n",
    "    \"75098\",\n",
    "    \"75104\",\n",
    "    \"75116\",\n",
    "    \"75126\",\n",
    "    \"75166\",\n",
    "    \"75182\",\n",
    "    \"75201\",\n",
    "    \"75202\",\n",
    "    \"75203\",\n",
    "    \"75204\",\n",
    "    \"75205\",\n",
    "    \"75206\",\n",
    "    \"75207\",\n",
    "    \"75208\",\n",
    "    \"75209\",\n",
    "    \"75210\",\n",
    "    \"75211\",\n",
    "    \"75212\",\n",
    "    \"75214\",\n",
    "    \"75215\",\n",
    "    \"75216\",\n",
    "    \"75217\",\n",
    "    \"75218\",\n",
    "    \"75219\",\n",
    "    \"75220\",\n",
    "    \"75221\",\n",
    "    \"75222\",\n",
    "    \"75223\",\n",
    "    \"75224\",\n",
    "    \"75225\",\n",
    "    \"75226\",\n",
    "    \"75227\",\n",
    "    \"75228\",\n",
    "    \"75229\",\n",
    "    \"75230\",\n",
    "    \"75231\",\n",
    "    \"75232\",\n",
    "    \"75233\",\n",
    "    \"75234\",\n",
    "    \"75235\",\n",
    "    \"75236\",\n",
    "    \"75237\",\n",
    "    \"75238\",\n",
    "    \"75240\",\n",
    "    \"75241\",\n",
    "    \"75242\",\n",
    "    \"75243\",\n",
    "    \"75244\",\n",
    "    \"75246\",\n",
    "    \"75247\",\n",
    "    \"75248\",\n",
    "    \"75249\",\n",
    "    \"75250\",\n",
    "    \"75251\",\n",
    "    \"75252\",\n",
    "    \"75253\",\n",
    "    \"75254\",\n",
    "    \"75260\",\n",
    "    \"75262\",\n",
    "    \"75263\",\n",
    "    \"75264\",\n",
    "    \"75265\",\n",
    "    \"75266\",\n",
    "    \"75267\",\n",
    "    \"75270\",\n",
    "    \"75277\",\n",
    "    \"75285\",\n",
    "    \"75287\",\n",
    "    \"75301\",\n",
    "    \"75303\",\n",
    "    \"75312\",\n",
    "    \"75313\",\n",
    "    \"75315\",\n",
    "    \"75320\",\n",
    "    \"75336\",\n",
    "    \"75339\",\n",
    "    \"75342\",\n",
    "    \"75354\",\n",
    "    \"75355\",\n",
    "    \"75356\",\n",
    "    \"75357\",\n",
    "    \"75359\",\n",
    "    \"75360\",\n",
    "    \"75367\",\n",
    "    \"75370\",\n",
    "    \"75371\",\n",
    "    \"75372\",\n",
    "    \"75373\",\n",
    "    \"75374\",\n",
    "    \"75376\",\n",
    "    \"75378\",\n",
    "    \"75379\",\n",
    "    \"75380\",\n",
    "    \"75382\",\n",
    "    \"75389\",\n",
    "    \"75390\",\n",
    "    \"75392\",\n",
    "    \"75393\",\n",
    "    \"75394\",\n",
    "    \"75395\",\n",
    "    \"75397\",\n",
    "    \"75398\",\n",
    "    \"76217\",\n",
    "]\n",
    "\n",
    "# plug in known area names from earlier work, fill the remainder as \"Dallas\"\n",
    "dallas_postal_dict = {}\n",
    "for zipcode in dallas_zip_codes:\n",
    "    if zipcode in dallas_area_postal_codes.keys():\n",
    "        dallas_postal_dict[zipcode] = dallas_area_postal_codes[zipcode]\n",
    "    else:\n",
    "        dallas_postal_dict[zipcode] = \"Dallas\"\n",
    "\n",
    "# combine two dicts, preferring dallas_area_postal_codes\n",
    "tx_zip_codes = {**dallas_postal_dict, **dallas_area_postal_codes}\n",
    "\n",
    "# run the code below to do the actual scrape:\n",
    "\n",
    "for postal_code, area in tx_zip_codes.items():\n",
    "     extract_listings(postal_code, area)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(nested_list):\n",
    "    return [item for sublist in nested_list for item in sublist]\n",
    "\n",
    "def load_listing(postal_code, area):\n",
    "    extracted_path = os.path.join(DTYPE, 'extracted')\n",
    "    with open(os.path.join(extracted_path, '{}-{}.json'.format(postal_code, area)), 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def load_all_listings(postal_codes):\n",
    "    zip_codes = {}\n",
    "    for postal_code, area in postal_codes.items():\n",
    "        name = \"{}_{}\".format(postal_code, area)\n",
    "        try:\n",
    "            zip_codes[name] = load_listing(postal_code, area)\n",
    "        except FileNotFoundError:\n",
    "            pass\n",
    "    return zip_codes\n",
    "\n",
    "zip_codes = load_all_listings(tx_zip_codes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### explore structure of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_codes['75023_Plano'][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[listing['price'] for listing in zip_codes['75023_Plano']][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_matches_to(term, limit=3):\n",
    "    l = [\n",
    "        [\n",
    "            \"\\n\".join([listing['title'], listing['price'], listing['href']] + listing['description']) for listing in zipcode \n",
    "            if term in \" \".join(listing['description']).lower() and int(listing['price'][1:]) > 1\n",
    "        ] for zipcode in zip_codes.values()\n",
    "    ]\n",
    "    i=0\n",
    "    for item in flatten(l):\n",
    "        print(item)\n",
    "        print()\n",
    "        i+=1\n",
    "        if limit:\n",
    "            if i==limit:\n",
    "                break\n",
    "\n",
    "print_matches_to('voucher')\n",
    "print_matches_to('section 8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_matches_to(term):\n",
    "    l = [\n",
    "        [\n",
    "            listing['href'] for listing in zipcode \n",
    "            if term in \" \".join(listing['description']).lower() and int(listing['price'][1:]) > 1\n",
    "        ] for zipcode in zip_codes.values()\n",
    "    ]\n",
    "    return flatten(l)\n",
    "\n",
    "print(\"voucher:\", find_matches_to(\"voucher\"))\n",
    "# print(\"section 8:\", find_matches_to(\"section 8\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_codes.keys()\n",
    "len(zip_codes['75025_Plano'])\n",
    "sum([len(zip_codes[where]) for where in zip_codes])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### convert to pandas DataFrame for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_nested(field):\n",
    "    return flatten(\n",
    "        [\n",
    "            [\n",
    "                listing[field] for listing in zip_codes[where]\n",
    "            ] for where in zip_codes\n",
    "        ]\n",
    "    )\n",
    "\n",
    "raw_df = pd.DataFrame({\n",
    "    \"href\":extract_nested('href'),\n",
    "    \"title\":extract_nested('title'),\n",
    "    \"price\":extract_nested('price'),\n",
    "    \"description\":extract_nested('description'),\n",
    "    \"duplicates\":extract_nested('duplicates')\n",
    "})\n",
    "\n",
    "raw_df['city'] = flatten([\n",
    "    np.tile(town, count) for town, count in [\n",
    "        (key.split('_')[1], len(zip_codes[key])) for key in zip_codes.keys()\n",
    "    ]\n",
    "])\n",
    "\n",
    "raw_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## featurize\n",
    "\n",
    "eventually the featurization steps should be unit tested and combined in a pipeline, but for now it is still unclear which steps will be necessary down the line and which will not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_href(url):\n",
    "    full_title = url.split('/')[-2]\n",
    "    city = full_title.split('-')[0] # will capture only 1st word, e.g. \"Grand Prairie\" --> \"Grand\"\n",
    "    title = \"-\".join(full_title.split('-')[1:]) # will capture any spillover from city, e.g. \"Prairie\"\n",
    "    c_id = url.split('/')[-1].split('.')[0]\n",
    "    return city, title, c_id\n",
    "\n",
    "parse_href('https://dallas.craigslist.org/dal/apa/d/richardson-luxury-second-chance/7112620971.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_title(url):\n",
    "    return url.split('/')[-2]\n",
    "\n",
    "# def parse_city(url):\n",
    "#     # will capture only 1st word, e.g. \"Grand Prairie\" --> \"Grand\"\n",
    "#     return _parse_title(url).split('-')[0]\n",
    "# \n",
    "# def parse_title(url):\n",
    "#     return \"-\".join(_parse_title(url).split('-')[1:])\n",
    "\n",
    "def parse_id(url):\n",
    "    return url.split('/')[-1].split('.')[0]\n",
    "\n",
    "def flatten_description(description):\n",
    "    return \"\\n\".join(description)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = raw_df.copy()\n",
    "\n",
    "new_df['parsed_id'] = raw_df['href'].apply(parse_id)\n",
    "new_df = new_df.loc[~new_df['parsed_id'].duplicated()]\n",
    "\n",
    "new_df['title'] = new_df['title'].str.lower()\n",
    "new_df['parsed_title'] = raw_df['href'].apply(parse_title).str.lower()\n",
    "new_df['description'] = new_df['description'].apply(flatten_description).str.lower()\n",
    "new_df['description_oneline'] = new_df['description'].str.replace(\"\\n\", \" \")\n",
    "\n",
    "new_df['n_dupes'] = new_df['duplicates'].apply(len)\n",
    "\n",
    "# price: chop off the '$' and convert to int\n",
    "new_df['price'] = raw_df['price'].apply(lambda x: int(x[1:]))\n",
    "\n",
    "new_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(new_df['description'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(new_df.loc[new_df['price']<5000, 'price'], bins=50)\n",
    "plt.xlabel('price')\n",
    "plt.ylabel('count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for city in np.sort(new_df['city'].unique()):\n",
    "    plt.hist(\n",
    "        new_df.loc[\n",
    "            (new_df['price']<5000) & (new_df['city']==city),\n",
    "            'price'\n",
    "        ],\n",
    "        bins=50)\n",
    "    plt.xlabel('price')\n",
    "    plt.xlim((0, 5000))\n",
    "    plt.ylabel('count')\n",
    "    plt.title(city)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### featurize bedrooms/bathrooms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_num = '\\d{1,10}\\.?\\d*'\n",
    "\n",
    "bed_match = new_df[\"description\"].apply(\n",
    "    lambda x: re.search(\n",
    "        \"({} (bed|bd)|(bed|beds|bd|bds|bedroom|bedrooms|bdrm|bdrms): {})\".format(re_num, re_num), x\n",
    "    )\n",
    ")\n",
    "\n",
    "bath_match = new_df[\"description\"].apply(\n",
    "    lambda x: re.search(\n",
    "        \"({} bath|(bath|bathroom|baths|bathrooms|bth|bths): {})\".format(re_num, re_num), x\n",
    "    )\n",
    ")\n",
    "\n",
    "def return_matches(match):\n",
    "    try:\n",
    "        return match.group(0)\n",
    "    except AttributeError:\n",
    "        return None\n",
    "\n",
    "def strip_text(string):\n",
    "    try:\n",
    "        match = re.search(\"\\d{1,10}\\.?\\d*\", string)\n",
    "    except TypeError:\n",
    "        return None\n",
    "    return float(return_matches(match))\n",
    "\n",
    "new_df['bedrooms'] = bed_match.apply(return_matches).apply(strip_text)\n",
    "new_df['bathrooms'] = bath_match.apply(return_matches).apply(strip_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(\n",
    "    new_df['bedrooms'], \n",
    "    bins=np.arange(1, max(new_df['bedrooms'].dropna()), .5)\n",
    ")\n",
    "plt.title('beds')\n",
    "plt.show()\n",
    "\n",
    "plt.hist(\n",
    "    new_df['bathrooms'], \n",
    "    bins=np.arange(1, max(new_df['bathrooms'].dropna()), 0.5)\n",
    ")\n",
    "plt.title('baths')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mult-unit listings currently return only a single value for bed/bath counts\n",
    "print(new_df.loc[new_df['parsed_id']=='7107475787', 'description'].values[0])\n",
    "# TODO: capture all matches, not just the first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### duplicates\n",
    "need to address these esp. if doing supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.loc[new_df['description'].duplicated(keep=False)].sort_values('description')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mark posts that have duplicates,\n",
    "# then deduplicate without attending to raw 'duplicates' column for now\n",
    "\n",
    "# CAUTION: mutates in place. Avoid this pattern in production.\n",
    "\n",
    "new_df['has_repost'] = False\n",
    "new_df.loc[new_df['description'].duplicated(keep=False), 'has_repost'] = True\n",
    "\n",
    "new_df = new_df.loc[\n",
    "    ~new_df['description'].duplicated()\n",
    "]\n",
    "\n",
    "new_df = new_df.drop(columns=['duplicates', 'n_dupes'])\n",
    "new_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spanglish = stopwords.words('spanish') + stopwords.words('english')\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    stop_words=spanglish, \n",
    "    token_pattern=\"[a-z]{2,}\"\n",
    ")\n",
    "\n",
    "X = vectorizer.fit_transform(new_df['description_oneline'])\n",
    "\n",
    "print(X.shape)\n",
    "print(vectorizer.get_feature_names()[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some of those tokens look surprising\n",
    "\n",
    "surprise_term = 'abacus'\n",
    "for entry in new_df['description_oneline']:\n",
    "    if surprise_term in entry:\n",
    "        print(entry)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering\n",
    "use tf-idf scores to cluster the postings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=2).fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the class labels\n",
    "pd.Series(kmeans.labels_).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.loc[kmeans.labels_==1].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### understand the clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what are the distinctive terms in the second (smaller) cluster?\n",
    "pd.Series(\n",
    "    vectorizer.get_feature_names()\n",
    ").loc[\n",
    "    kmeans.cluster_centers_[1]>0.1\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### is \"locator\" the distinguishing characteristic?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.loc[\n",
    "    (kmeans.labels_==1), 'description_oneline'\n",
    "].str.contains(\"locator\").value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.loc[\n",
    "    (kmeans.labels_==0), 'description_oneline'\n",
    "].str.contains(\"locator\").value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "proportions appear different, and we could use a contingency table to test for significance, but there are probably better clusterings to be found"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Silhouette analysis\n",
    "what value of K maximizes silhouette score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html\n",
    "\n",
    "range_n_clusters = [2, 3, 4, 5, 6]\n",
    "\n",
    "for n_clusters in range_n_clusters:\n",
    "    fig, ax1 = plt.subplots(1, 1)\n",
    "    fig.set_size_inches(18, 7)\n",
    "\n",
    "    clusterer = KMeans(n_clusters=n_clusters, random_state=10)\n",
    "    cluster_labels = clusterer.fit_predict(X)\n",
    "\n",
    "    # The silhouette_score gives the average value for all the samples.\n",
    "    silhouette_avg = silhouette_score(X, cluster_labels)\n",
    "    print(\"For n_clusters =\", n_clusters,\n",
    "          \"The average silhouette_score is:\", silhouette_avg)\n",
    "\n",
    "    # Compute the silhouette scores for each sample\n",
    "    sample_silhouette_values = silhouette_samples(X, cluster_labels)\n",
    "\n",
    "    y_lower = 10\n",
    "    for i in range(n_clusters):\n",
    "        # Aggregate the silhouette scores for samples belonging to\n",
    "        # cluster i, and sort them\n",
    "        ith_cluster_silhouette_values = \\\n",
    "            sample_silhouette_values[cluster_labels == i]\n",
    "\n",
    "        ith_cluster_silhouette_values.sort()\n",
    "\n",
    "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "\n",
    "        color = cm.nipy_spectral(float(i) / n_clusters)\n",
    "        ax1.fill_betweenx(np.arange(y_lower, y_upper),\n",
    "                          0, ith_cluster_silhouette_values,\n",
    "                          facecolor=color, edgecolor=color, alpha=0.7)\n",
    "\n",
    "        # Label the silhouette plots with their cluster numbers at the middle\n",
    "        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "\n",
    "        # Compute the new y_lower for next plot\n",
    "        y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "\n",
    "    ax1.set_title(\"The silhouette plot for the various clusters.\")\n",
    "    ax1.set_xlabel(\"The silhouette coefficient values\")\n",
    "    ax1.set_ylabel(\"Cluster label\")\n",
    "\n",
    "    # The vertical line for average silhouette score of all the values\n",
    "    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
    "    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "\n",
    "    plt.suptitle((\"Silhouette analysis for KMeans clustering\"\n",
    "                  \"with n_clusters = %d\" % n_clusters),\n",
    "                 fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* increasing number of clusters increases \"fit\" of postings within their assigned cluster\n",
    "* no clustering yet solves the problem of one or more large \"miscellaneous\" clusters with poor fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### try direct match to \"locator\" in the posting? \n",
    "##### probably should have started with this approach as a baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df['description_oneline'].str.contains('locator').value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "should spot-check some of these postings, in case legitimate posts use the word \"locator\"\n",
    "\n",
    "future direction:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Learning\n",
    "plan:\n",
    "* hand-label spammy posts from a random subset of the data?\n",
    "* grab processed features from new_df (price, n_beds, n_baths, has_repost)\n",
    "* maybe include tf-idf scores, consider either dimensionality reduction or regularization given the number of features\n",
    "* fit logistic regression, random forest, xgboost models\n",
    "* compare precision, recall after determining whether false positives or false negatives are more problematic\n",
    "\n",
    "potential issues:\n",
    "* spanish/english appearing within/across posts\n",
    "* spammers must balance the tradeoff between evading automatic filters and still getting people to click through or call\n",
    "\n",
    "big picture:\n",
    "* will filtering out these spammy posts help evaluate discrimination against housing vouchers?\n",
    "* are there differences by geography?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "housing-discrim",
   "language": "python",
   "name": "housing-discrim"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
